# -*- coding: utf-8 -*-
"""CREDIT CARD FRAUD DETECTION MODEL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V5wh6Q-7pkqcAPZbCrWNBaQ-ybufYJ2y
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
df_detect = pd.read_csv('/content/creditcard.csv')
df_detect

#explore Data
print (df_detect.info())

#Find Missing Values
missing_values = df_detect.isnull().sum()
print(missing_values)

#Checking if Dataset is IMBALANCED or BALANCED

# Calculate percentages
class_percentages = df_detect['Class'].value_counts(normalize=True) * 100

# Print the count of each class
print(df_detect['Class'].value_counts())

# Also, print the percentage distribution
print(df_detect['Class'].value_counts(normalize=True) * 100)

no_fraud_percent = round(class_percentages.get(0, 0), 2)
fraud_percent = round(class_percentages.get(1, 0), 2)

print('\n')
print(f"Percentage of No Fraudulent Transactions: {no_fraud_percent}%")
print(f"Percentage of Fraudulent Transactions: {fraud_percent}%")

#Normalize Amount and Time
from sklearn.preprocessing import MinMaxScaler
print(df_detect['Class'].value_counts(normalize=True))
print(df_detect['Amount'].value_counts(normalize=True))
print(df_detect['Time'].value_counts(normalize=True))


scaler = MinMaxScaler()
df_detect['Normalized_Amount'] = scaler.fit_transform(df_detect['Amount'].values.reshape(-1, 1))
df_detect['Normalized_Time'] = scaler.fit_transform(df_detect['Time'].values.reshape(-1, 1))
print(df_detect[['Normalized_Amount','Normalized_Time']].head())

#Balancing Dataset using undersampling

# Separate majority and minority classes
df_majority = df_detect[df_detect['Class'] == 0]
df_minority = df_detect[df_detect['Class'] == 1]

# Downsample majority class to the size of minority class
df_majority_downsampled = df_majority.sample(n=len(df_minority), random_state=42)

# Combine minority class with downsampled majority class
df_balanced = pd.concat([df_majority_downsampled, df_minority])

# Shuffle the new balanced dataframe
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Check new class distribution
print(df_balanced['Class'].value_counts())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# 1. Define features and target
X = df_balanced[['Normalized_Amount', 'Normalized_Time']]
y = df_balanced['Class']

# 2. Split into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Train Logistic Regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# 4. Predict on test data
y_pred = model.predict(X_test)

# 5. Show performance
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Confusion Matrix Analysis

True Positives (56): Correctly predicted frauds

True Negatives (56): Correctly predicted no-frauds

False Positives (54): Predicted fraud but was not fraud

False Negatives (31): Missed fraud cases

Conclusion


1  The logistic regression model didn't work very well — it got confused between fraud and non-fraud cases.

2 Since it made a lot of mistakes, we’ll try a better model like Random Forest to improve the results.


"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# 1. Define features and target
X = df_balanced[['Normalized_Amount', 'Normalized_Time']]  # Add more features if needed
y = df_balanced['Class']

# 2. Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Train the Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# 4. Predict on test data
y_pred = rf_model.predict(X_test)

# 5. Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""CONCLUSION

* Explored and preprocessed the data, normalizing key features and handling class imbalance with downsampling.
* Logistic Regression model showed limited performance with many misclassifications.
* Random Forest model improved accuracy and better detected fraud cases compared to Logistic Regression.
* Downsampling helped balance classes but reduced data from the majority class.
* Future improvements could include using oversampling techniques like SMOTE and adding more features.
* Random Forest serves as a strong baseline for fraud detection in this dataset.

"""
